{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy of RNN final.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAvXTuQ-Zu5C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "e1a2d561-9a5d-42bc-e38a-469bac6da152"
      },
      "source": [
        "import collections\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        " \n",
        "import tensorflow as tf\n",
        " \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9b9878006d37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy5vG47PZu5E"
      },
      "source": [
        "dataset_dir = pathlib.Path(\"F:\\Project\\Dataset\")\n",
        "train_dir = dataset_dir/'Train'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDCUSxd4Zu5F",
        "outputId": "233e4990-da19-4473-f380-7df6afc8a848"
      },
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = preprocessing.text_dataset_from_directory(\n",
        "    train_dir,\n",
        "    batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 14000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCATIF7eZu5G",
        "outputId": "4db6c412-82f2-4867-9af3-56092692a88a"
      },
      "source": [
        "vali_dir=dataset_dir/'Validate'\n",
        "raw_val_ds = preprocessing.text_dataset_from_directory(\n",
        "    vali_dir,\n",
        "    batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkR4LiYlZu5G",
        "outputId": "51efb793-8909-45e2-9f26-305f78e77640"
      },
      "source": [
        "test_dir = dataset_dir/'Test'\n",
        "raw_test_ds = preprocessing.text_dataset_from_directory(\n",
        "    test_dir, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3001 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZteE32jsZu5G"
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "\n",
        "binary_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KznfwFr0Zu5H"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 250\n",
        "\n",
        "int_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ETZo3BGZu5H"
      },
      "source": [
        "train_text = raw_train_ds.map(lambda text, labels: text)\n",
        "binary_vectorize_layer.adapt(train_text)\n",
        "int_vectorize_layer.adapt(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM-ZX811Zu5H"
      },
      "source": [
        "def binary_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return binary_vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU-s3r2EZu5I"
      },
      "source": [
        "def int_vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return int_vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RhFYQWQZu5I",
        "outputId": "fe87b008-2d4c-4042-ed84-6797c3ee7e97"
      },
      "source": [
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_tweet, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Tweet\", first_tweet)\n",
        "print(\"Label\", first_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tweet tf.Tensor(b' do you cuss alot?', shape=(), dtype=string)\n",
            "Label tf.Tensor(0, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clvR65JeZu5I"
      },
      "source": [
        "binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n",
        "binary_val_ds = raw_val_ds.map(binary_vectorize_text)\n",
        "binary_test_ds = raw_test_ds.map(binary_vectorize_text)\n",
        "\n",
        "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
        "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
        "int_test_ds = raw_test_ds.map(int_vectorize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSnzKDkYZu5J",
        "outputId": "dfc2c9ca-66b1-4765-a98e-69884cdad13a"
      },
      "source": [
        "binary_model = tf.keras.Sequential([layers.Dense(4)])\n",
        "binary_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "history = binary_model.fit(\n",
        "    binary_train_ds, validation_data=binary_val_ds, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "438/438 [==============================] - 23s 51ms/step - loss: 1.0445 - accuracy: 0.6259 - val_loss: 0.6852 - val_accuracy: 0.7687\n",
            "Epoch 2/10\n",
            "438/438 [==============================] - 7s 15ms/step - loss: 0.6288 - accuracy: 0.8110 - val_loss: 0.5800 - val_accuracy: 0.7950\n",
            "Epoch 3/10\n",
            "438/438 [==============================] - 7s 15ms/step - loss: 0.5179 - accuracy: 0.8469 - val_loss: 0.5336 - val_accuracy: 0.8030\n",
            "Epoch 4/10\n",
            "438/438 [==============================] - 7s 15ms/step - loss: 0.4573 - accuracy: 0.8595 - val_loss: 0.5061 - val_accuracy: 0.8103\n",
            "Epoch 5/10\n",
            "438/438 [==============================] - 7s 15ms/step - loss: 0.4166 - accuracy: 0.8673 - val_loss: 0.4869 - val_accuracy: 0.8150\n",
            "Epoch 6/10\n",
            "438/438 [==============================] - 7s 15ms/step - loss: 0.3862 - accuracy: 0.8777 - val_loss: 0.4722 - val_accuracy: 0.8213\n",
            "Epoch 7/10\n",
            "438/438 [==============================] - 7s 15ms/step - loss: 0.3620 - accuracy: 0.8880 - val_loss: 0.4585 - val_accuracy: 0.8233\n",
            "Epoch 8/10\n",
            "438/438 [==============================] - 7s 15ms/step - loss: 0.3423 - accuracy: 0.8945 - val_loss: 0.4481 - val_accuracy: 0.8243\n",
            "Epoch 9/10\n",
            "438/438 [==============================] - 7s 15ms/step - loss: 0.3254 - accuracy: 0.9001 - val_loss: 0.4373 - val_accuracy: 0.8227\n",
            "Epoch 10/10\n",
            "438/438 [==============================] - 7s 15ms/step - loss: 0.3097 - accuracy: 0.9060 - val_loss: 0.4269 - val_accuracy: 0.8257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EoPaN9tZu5J"
      },
      "source": [
        "def create_model(vocab_size, num_labels):\n",
        "  model = tf.keras.Sequential([\n",
        "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
        "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
        "      layers.GlobalMaxPooling1D(),\n",
        "      layers.Dense(num_labels)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxzyUOaxZu5K",
        "outputId": "861c8466-de49-4a48-9da9-d4822dc7620e"
      },
      "source": [
        "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=2)\n",
        "int_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "438/438 [==============================] - 12s 26ms/step - loss: 0.5742 - accuracy: 0.6862 - val_loss: 0.4590 - val_accuracy: 0.8087\n",
            "Epoch 2/5\n",
            "438/438 [==============================] - 11s 26ms/step - loss: 0.3178 - accuracy: 0.8788 - val_loss: 0.3426 - val_accuracy: 0.8570\n",
            "Epoch 3/5\n",
            "438/438 [==============================] - 12s 26ms/step - loss: 0.1645 - accuracy: 0.9469 - val_loss: 0.3059 - val_accuracy: 0.8940\n",
            "Epoch 4/5\n",
            "438/438 [==============================] - 11s 26ms/step - loss: 0.0835 - accuracy: 0.9744 - val_loss: 0.3007 - val_accuracy: 0.9030\n",
            "Epoch 5/5\n",
            "438/438 [==============================] - 11s 26ms/step - loss: 0.0467 - accuracy: 0.9854 - val_loss: 0.3310 - val_accuracy: 0.9053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xrt55bfHZu5K",
        "outputId": "fb75078e-a27e-4119-bef8-9428ac8411f9"
      },
      "source": [
        "print(\"Linear model on binary vectorized data:\")\n",
        "print(binary_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear model on binary vectorized data:\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 4)                 40004     \n",
            "=================================================================\n",
            "Total params: 40,004\n",
            "Trainable params: 40,004\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H02rggedZu5L",
        "outputId": "4b9212f5-56b8-43bc-e56a-acae4f167519"
      },
      "source": [
        "print(\"ConvNet model on int vectorized data:\")\n",
        "print(int_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet model on int vectorized data:\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          640064    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 64)          20544     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 660,738\n",
            "Trainable params: 660,738\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1Mv-G2CDZu5L",
        "outputId": "1b942c8f-e637-47b8-f399-8cdcf2ccdb85"
      },
      "source": [
        "binary_loss, binary_accuracy = binary_model.evaluate(binary_test_ds)\n",
        "int_loss, int_accuracy = int_model.evaluate(int_test_ds)\n",
        "\n",
        "print(\"Binary model accuracy: {:2.2%}\".format(binary_accuracy))\n",
        "print(\"Int model accuracy: {:2.2%}\".format(int_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94/94 [==============================] - 17s 162ms/step - loss: 0.4273 - accuracy: 0.8257\n",
            "94/94 [==============================] - 1s 13ms/step - loss: 0.3689 - accuracy: 0.8940\n",
            "Binary model accuracy: 82.57%\n",
            "Int model accuracy: 89.40%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhwvygqbZu5M",
        "outputId": "2bf47306-e74f-428c-a076-90175dfa4448"
      },
      "source": [
        "export_model = tf.keras.Sequential(\n",
        "    [int_vectorize_layer, int_model,\n",
        "     layers.Activation('sigmoid')])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(\"Accuracy: {:2.2%}\".format(int_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94/94 [==============================] - 2s 13ms/step - loss: 0.3874 - accuracy: 0.8923\n",
            "Accuracy: 89.40%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuiwklNHZu5M"
      },
      "source": [
        "def get_string_labels(predicted_scores_batch):\n",
        "  predicted_int_labels = tf.argmax(predicted_scores_batch, axis=1)\n",
        "  predicted_labels = tf.gather(raw_train_ds.class_names, predicted_int_labels)\n",
        "  return predicted_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw-Ews-8Zu5M",
        "outputId": "42862a35-60dc-47b6-c79c-b2ce32ae4806"
      },
      "source": [
        "inputs = [\n",
        "    \"Fuck you its done\",  # python\n",
        "    \"whatsup ma dude\",  # java\n",
        "]\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = get_string_labels(predicted_scores)\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"tweet: \", input)\n",
        "  print(\"Predicted label: \", label.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tweet:  Fuck you its done\n",
            "Predicted label:  b'Trolls'\n",
            "tweet:  whatsup ma dude\n",
            "Predicted label:  b'Non-trolls'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}